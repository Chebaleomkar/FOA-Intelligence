# üöÄ FOA-Intelligence: Implementation Plan & GSoC Preparation Guide

## üìã Table of Contents
1. [Project Overview & Analysis](#1-project-overview--analysis)
2. [Architecture Design](#2-architecture-design)
3. [Phase-wise Implementation Plan](#3-phase-wise-implementation-plan)
4. [Screening Task Strategy](#4-screening-task-strategy)
5. [Technical Deep Dives](#5-technical-deep-dives)
6. [GSoC Proposal Tips](#6-gsoc-proposal-tips)
7. [Timeline & Milestones](#7-timeline--milestones)

---

## 1. Project Overview & Analysis

### What This Project Is
You're building an **open-source pipeline** that:
1. **Ingests** Funding Opportunity Announcements (FOAs) from public APIs (Grants.gov, NSF)
2. **Extracts** structured fields (title, agency, dates, eligibility, etc.)
3. **Tags** them semantically using NLP (rule-based + embeddings + optional LLM)
4. **Exports** clean JSON + CSV outputs for downstream grant matching

### Why This Project is a Good Fit for You
- It's a **data engineering + NLP pipeline** ‚Äî very practical and impressive
- Uses Python, web scraping, embeddings, and structured data ‚Äî skills you already have
- **Intermediate difficulty** ‚Äî achievable in GSoC timeline with stretch goals for ambition
- Clear deliverables ‚Äî mentors can evaluate progress objectively

### Key Insight: Use APIs, Not Scraping!
**Critical discovery**: Both Grants.gov and NSF provide **free, public REST APIs** with **no authentication required** for searching and fetching funding opportunities. This is far more reliable than HTML scraping.

| Source | API Endpoint | Auth Required? | Data Format |
|--------|-------------|----------------|-------------|
| **Grants.gov** | `https://api.grants.gov/v1/api/search2` | ‚ùå No | JSON |
| **Grants.gov** | `https://api.grants.gov/v1/api/fetchOpportunity` | ‚ùå No | JSON |
| **NSF Awards** | `https://api.nsf.gov/services/v1/awards.json` | ‚ùå No | JSON/XML |

> **Strategy**: Use APIs as primary ingestion method, fall back to HTML/PDF scraping only when API data is incomplete (e.g., full program descriptions in PDFs).

---

## 2. Architecture Design

### High-Level Pipeline Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FOA Intelligence Pipeline                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   INGESTION   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  EXTRACTION  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  SEMANTIC TAGGING     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Grants.gov ‚îÇ    ‚îÇ ‚Ä¢ Field      ‚îÇ    ‚îÇ ‚Ä¢ Rule-based tags     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   API        ‚îÇ    ‚îÇ   Extraction ‚îÇ    ‚îÇ ‚Ä¢ Embedding similarity‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ NSF API    ‚îÇ    ‚îÇ ‚Ä¢ Date       ‚îÇ    ‚îÇ ‚Ä¢ LLM classification  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ PDF Parser ‚îÇ    ‚îÇ   Parsing    ‚îÇ    ‚îÇ   (stretch goal)      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ HTML Parse ‚îÇ    ‚îÇ ‚Ä¢ Schema     ‚îÇ    ‚îÇ ‚Ä¢ Ontology alignment  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ   Validation ‚îÇ    ‚îÇ                       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                      ‚îÇ              ‚îÇ
‚îÇ                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                                          ‚îÇ    STORAGE & EXPORT   ‚îÇ  ‚îÇ
‚îÇ                                          ‚îÇ                       ‚îÇ  ‚îÇ
‚îÇ                                          ‚îÇ ‚Ä¢ JSON export         ‚îÇ  ‚îÇ
‚îÇ                                          ‚îÇ ‚Ä¢ CSV export          ‚îÇ  ‚îÇ
‚îÇ                                          ‚îÇ ‚Ä¢ Update workflow     ‚îÇ  ‚îÇ
‚îÇ                                          ‚îÇ ‚Ä¢ FAISS index (opt.)  ‚îÇ  ‚îÇ
‚îÇ                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Project Structure

```
FOA-Intelligence/
‚îú‚îÄ‚îÄ main.py                      # CLI entry point
‚îú‚îÄ‚îÄ requirements.txt             # Dependencies
‚îú‚îÄ‚îÄ README.md                    # Documentation
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ settings.py              # Configuration settings
‚îÇ   ‚îî‚îÄ‚îÄ ontology.yaml            # Controlled vocabulary/ontology
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py              # Abstract ingestion interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grants_gov.py        # Grants.gov API ingestion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nsf.py               # NSF API ingestion
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pdf_parser.py        # PDF text extraction
‚îÇ   ‚îú‚îÄ‚îÄ extraction/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ field_extractor.py   # Structured field extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py            # FOA schema (Pydantic model)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ normalizer.py        # Text/date normalization
‚îÇ   ‚îú‚îÄ‚îÄ tagging/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rule_based.py        # Keyword/regex rule-based tagger
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding_tagger.py  # Sentence-transformer similarity tagger
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_tagger.py        # Optional LLM-based tagger (stretch)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ontology.py          # Ontology loader + tag definitions
‚îÇ   ‚îú‚îÄ‚îÄ export/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ json_exporter.py     # JSON output
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ csv_exporter.py      # CSV output
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ evaluator.py         # Precision/recall/agreement metrics
‚îÇ       ‚îî‚îÄ‚îÄ gold_standard.json   # Hand-labeled evaluation set
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_ingestion.py
‚îÇ   ‚îú‚îÄ‚îÄ test_extraction.py
‚îÇ   ‚îú‚îÄ‚îÄ test_tagging.py
‚îÇ   ‚îî‚îÄ‚îÄ test_export.py
‚îú‚îÄ‚îÄ out/                         # Default output directory
‚îÇ   ‚îú‚îÄ‚îÄ foa.json
‚îÇ   ‚îî‚îÄ‚îÄ foa.csv
‚îú‚îÄ‚îÄ data/                        # Raw/cached data (gitignored)
‚îÇ   ‚îî‚îÄ‚îÄ cache/
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ DESIGN.md                # Design decisions
    ‚îú‚îÄ‚îÄ ONTOLOGY.md              # Ontology documentation
    ‚îî‚îÄ‚îÄ EVALUATION.md            # Evaluation methodology + results
```

---

## 3. Phase-wise Implementation Plan

### Phase 0: Foundation Setup (Week 0 ‚Äî Pre-GSoC) ‚¨ÖÔ∏è DO THIS NOW
> **Goal**: Set up project skeleton + complete the screening task

- [x] Read project description
- [ ] Set up Python project with virtual environment
- [ ] Create project directory structure
- [ ] Initialize git repository
- [ ] Define FOA schema (Pydantic model)
- [ ] **Complete screening task** (see Section 4)
- [ ] Write initial README.md
- [ ] Submit screening task + CV

### Phase 1: FOA Ingestion (GSoC Weeks 1‚Äì3)
> **Goal**: Build robust ingestion from 2+ public sources

#### Grants.gov Ingestion
- Implement API client for `search2` and `fetchOpportunity`
- Handle pagination (API returns paginated results)
- Cache raw API responses to `data/cache/`
- Rate limiting and error handling

#### NSF Ingestion
- Implement NSF Awards API client
- Query by program, date range, keywords
- Map NSF-specific fields to unified schema

#### PDF/HTML Fallback
- Use `PyPDF2`/`pdfminer` for PDF extraction when linked in API responses
- Use `BeautifulSoup` for any supplementary HTML pages
- Text cleaning and normalization

### Phase 2: Structured Extraction + Normalization (GSoC Weeks 3‚Äì5)
> **Goal**: Extract all required fields into a standardized schema

#### Schema Definition (Pydantic)
```python
class FOARecord(BaseModel):
    foa_id: str                    # Generated UUID if missing
    title: str
    agency: str
    open_date: Optional[date]      # ISO format
    close_date: Optional[date]     # ISO format
    eligibility: Optional[str]
    program_description: Optional[str]
    award_range_min: Optional[float]
    award_range_max: Optional[float]
    source_url: str
    source: str                    # "grants_gov" | "nsf"
    raw_text: Optional[str]        # Original text for tagging
    semantic_tags: List[SemanticTag] = []
    ingested_at: datetime
```

#### Normalization
- Date parsing: Handle multiple formats ‚Üí ISO 8601
- Agency name normalization (e.g., "NSF" vs "National Science Foundation")
- Currency/amount extraction from free text
- Text cleaning: Remove HTML entities, normalize whitespace

### Phase 3: Semantic Tagging (GSoC Weeks 5‚Äì8) ‚≠ê CORE ML WORK
> **Goal**: Tag FOAs with research domains, methods, populations, sponsor themes

#### 3a. Controlled Ontology Design
Create `config/ontology.yaml` with hierarchical tags:

```yaml
research_domains:
  - artificial_intelligence:
      synonyms: [AI, machine learning, deep learning, neural networks]
      children: [nlp, computer_vision, reinforcement_learning]
  - biomedical:
      synonyms: [health, medical, clinical, pharmaceutical]
      children: [genomics, epidemiology, mental_health]
  - environmental_science:
      synonyms: [climate, ecology, sustainability, earth science]
  - social_science:
      synonyms: [sociology, psychology, anthropology, economics]
  # ... more domains

methods:
  - computational_modeling
  - survey_research
  - field_study
  - experimental
  - mixed_methods

populations:
  - underserved_communities
  - veterans
  - youth
  - elderly
  - indigenous

sponsor_themes:
  - innovation
  - workforce_development
  - national_security
  - equity_and_inclusion
  - infrastructure
```

#### 3b. Rule-Based Tagger
- **Keyword matching**: Match ontology terms + synonyms against FOA text
- **Regex patterns**: Extract structured patterns (e.g., "ages 18‚Äì65")
- **Confidence scoring**: Based on term frequency and position

#### 3c. Embedding Similarity Tagger
- Use `sentence-transformers` (e.g., `all-MiniLM-L6-v2`) 
- Encode ontology tag descriptions as reference embeddings
- Encode FOA text (title + description) as query embeddings
- Compute cosine similarity ‚Üí assign tags above threshold
- This is the **key NLP differentiator** of the project

#### 3d. LLM-Assisted Classification (Stretch Goal)
- Use a free/cheap LLM API (Groq, Ollama, etc.)
- Prompt: "Given this FOA, classify into these ontology categories..."
- Use LLM as a third signal, ensemble with rule-based + embedding

#### Tag Fusion Strategy
```
Final Tags = Rule-Based ‚à© Embedding (agreement) 
           ‚à™ High-confidence from either method
           + Optional LLM verification for borderline cases
```

### Phase 4: Storage & Export (GSoC Weeks 8‚Äì9)
> **Goal**: Produce clean, reproducible outputs

- JSON export: Prettified, schema-validated
- CSV export: Flat format with tags as comma-separated values
- Incremental update workflow (detect new FOAs, skip already processed)
- FAISS vector index for similarity search (stretch goal)

### Phase 5: Evaluation (GSoC Weeks 9‚Äì10)
> **Goal**: Demonstrate tagging quality with metrics

- Hand-label **25‚Äì50 FOAs** as gold standard (you + a friend/mentor)
- Compute per-tag precision, recall, F1
- Inter-rater agreement (Cohen's kappa if 2+ annotators)
- Error analysis and improvement suggestions

### Phase 6: Documentation & Polish (GSoC Weeks 10‚Äì12)
> **Goal**: Make the project production-ready and well-documented

- Complete README with setup instructions
- API documentation
- Design decision document
- Reproducibility guide
- Final demo video/notebook

---

## 4. Screening Task Strategy üéØ

### Requirements Recap
Build `main.py` that:
```bash
python main.py --url "<FOA_URL>" --out_dir ./out
```

Outputs:
- `out/foa.json` ‚Äî Structured FOA data
- `out/foa.csv` ‚Äî Same data in CSV format

### Implementation Strategy

The screening task should demonstrate:
1. **Clean, modular code** ‚Äî Show software engineering skills
2. **Smart data extraction** ‚Äî Handle both API and webpage sources
3. **Rule-based tagging** ‚Äî Even basic tagging shows NLP awareness
4. **Proper error handling** ‚Äî Edge cases matter
5. **Documentation** ‚Äî README + inline comments

### Screening Task Flow

```
URL Input ‚Üí Detect Source (Grants.gov? NSF?) 
         ‚Üí Fetch via API (preferred) or scrape HTML
         ‚Üí Extract fields into schema
         ‚Üí Apply rule-based semantic tags
         ‚Üí Export JSON + CSV
```

### Key URLs to Support for Screening Task
- `https://www.grants.gov/search-results-detail/<OPPORTUNITY_ID>`
- `https://www.nsf.gov/awardsearch/showAward?AWD_ID=<AWARD_ID>`

### What Will Impress the Mentors
1. **Using the Grants.gov API** instead of raw scraping (shows research)
2. **Pydantic schema validation** (shows software engineering maturity)
3. **Even basic ontology tags** (shows you understand the NLP scope)
4. **Tests** (even 2-3 tests show professionalism)
5. **Clean git history** with meaningful commits

---

## 5. Technical Deep Dives

### 5.1 Grants.gov API Usage

```python
import requests

GRANTS_GOV_SEARCH_URL = "https://api.grants.gov/v1/api/search2"
GRANTS_GOV_FETCH_URL = "https://api.grants.gov/v1/api/fetchOpportunity"

def search_opportunities(keyword: str, page: int = 1):
    """Search Grants.gov for funding opportunities."""
    payload = {
        "keyword": keyword,
        "oppStatuses": "forecasted|posted",
        "sortBy": "openDate|desc",
        "rows": 25,
        "offset": (page - 1) * 25
    }
    response = requests.post(GRANTS_GOV_SEARCH_URL, json=payload)
    response.raise_for_status()
    return response.json()

def fetch_opportunity(opp_id: int):
    """Fetch detailed opportunity data by ID."""
    payload = {"oppId": opp_id}
    response = requests.post(GRANTS_GOV_FETCH_URL, json=payload)
    response.raise_for_status()
    return response.json()
```

### 5.2 NSF Awards API Usage

```python
NSF_API_URL = "https://api.nsf.gov/services/v1/awards.json"

def search_nsf_awards(keyword: str, start_date: str = None):
    """Search NSF awards API."""
    params = {
        "keyword": keyword,
        "printFields": "id,title,agency,startDate,expDate,abstractText,fundsObligatedAmt",
        "resultsPerPage": 25,
    }
    if start_date:
        params["dateStart"] = start_date  # Format: MM/DD/YYYY
    
    response = requests.get(NSF_API_URL, params=params)
    response.raise_for_status()
    return response.json()
```

### 5.3 Semantic Tagging with Embeddings

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class EmbeddingTagger:
    def __init__(self, ontology: dict, model_name: str = "all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
        self.ontology = ontology
        self._build_reference_embeddings()
    
    def _build_reference_embeddings(self):
        """Pre-compute embeddings for ontology terms."""
        self.tag_names = []
        self.tag_texts = []
        
        for category, tags in self.ontology.items():
            for tag in tags:
                name = tag if isinstance(tag, str) else list(tag.keys())[0]
                synonyms = tag[name].get("synonyms", []) if isinstance(tag, dict) else []
                text = f"{name} {' '.join(synonyms)}"
                self.tag_names.append(f"{category}/{name}")
                self.tag_texts.append(text)
        
        self.reference_embeddings = self.model.encode(self.tag_texts)
    
    def tag(self, text: str, threshold: float = 0.35) -> list:
        """Tag text using embedding similarity."""
        query_embedding = self.model.encode([text])
        similarities = cosine_similarity(query_embedding, self.reference_embeddings)[0]
        
        tags = []
        for i, sim in enumerate(similarities):
            if sim >= threshold:
                tags.append({
                    "tag": self.tag_names[i],
                    "confidence": float(sim),
                    "method": "embedding"
                })
        
        return sorted(tags, key=lambda x: x["confidence"], reverse=True)
```

### 5.4 FOA Schema (Pydantic)

```python
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import date, datetime
import uuid

class SemanticTag(BaseModel):
    tag: str                         # e.g., "research_domains/artificial_intelligence"
    confidence: float = Field(ge=0, le=1)
    method: str                      # "rule_based" | "embedding" | "llm"

class FOARecord(BaseModel):
    foa_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    title: str
    agency: str
    open_date: Optional[date] = None
    close_date: Optional[date] = None
    eligibility: Optional[str] = None
    program_description: Optional[str] = None
    award_range_min: Optional[float] = None
    award_range_max: Optional[float] = None
    source_url: str
    source: str                      # "grants_gov" | "nsf"
    semantic_tags: List[SemanticTag] = []
    ingested_at: datetime = Field(default_factory=datetime.utcnow)
```

---

## 6. GSoC Proposal Tips üìù

### What Mentors Look For
1. **Understanding of the problem** ‚Äî Show you know WHY this matters
2. **Technical depth** ‚Äî Demonstrate you've researched the APIs, NLP techniques
3. **Realistic timeline** ‚Äî Don't overcommit; include buffer weeks
4. **Previous work** ‚Äî Link to your relevant projects (NLP, scraping, embeddings)
5. **Screening task quality** ‚Äî This is your FIRST impression

### Proposal Structure (Recommended)
1. **Title & Synopsis** (1 paragraph)
2. **Problem Statement** (why this matters, who benefits)
3. **Proposed Solution** (architecture diagram, tech choices)
4. **Implementation Plan** (weekly breakdown with milestones)
5. **Technical Approach**
   - Data sources + API strategy
   - Schema design
   - Tagging methodology (rule-based vs embedding vs LLM)
   - Evaluation strategy
6. **Stretch Goals** (show ambition: FAISS, CLI search, additional sources)
7. **About Me** (relevant skills, projects, contributions)
8. **Timeline** (aligned with GSoC schedule)

### Common Mistakes to Avoid
- ‚ùå Copying the project description verbatim
- ‚ùå Not completing the screening task
- ‚ùå Proposing a timeline without implementation details
- ‚ùå Not showing understanding of the NLP/ML components
- ‚ùå Not linking relevant previous work

### How to Stand Out
- ‚úÖ Show a working screening task with **clean code + tests**
- ‚úÖ Propose a clear, well-defined ontology (not vague)
- ‚úÖ Demonstrate embedding similarity with a small example
- ‚úÖ Show awareness of edge cases (missing dates, multi-agency FOAs)
- ‚úÖ Contribute to HumanAI Foundation's GitHub before proposing

---

## 7. Timeline & Milestones

### Pre-GSoC (NOW ‚Üí March)
| Week | Task | Deliverable |
|------|------|------------|
| Week 1 | Set up project + understand APIs | Working API test scripts |
| Week 2 | Build screening task | `main.py` + `foa.json` + `foa.csv` |
| Week 3 | Polish + write README | Submit screening task + CV |
| Week 4 | Write GSoC proposal | Draft proposal |
| Week 5 | Review + iterate proposal | Final proposal submission |

### GSoC Period (June‚ÄìAugust, ~12 weeks)
| Phase | Weeks | Focus | Deliverable |
|-------|-------|-------|-------------|
| 1 | 1‚Äì3 | Ingestion pipeline | 2 working source connectors |
| 2 | 3‚Äì5 | Extraction + normalization | Schema-validated FOA records |
| 3 | 5‚Äì8 | Semantic tagging | Rule-based + embedding taggers |
| 4 | 8‚Äì9 | Export + storage | JSON/CSV export + update workflow |
| 5 | 9‚Äì10 | Evaluation | Gold standard + metrics report |
| 6 | 10‚Äì12 | Docs + polish + stretch goals | Production-ready pipeline |

---

## üéØ Immediate Next Steps

1. **Set up the project** ‚Äî Create virtual env, install dependencies
2. **Test the APIs** ‚Äî Make your first Grants.gov and NSF API calls
3. **Build the screening task** ‚Äî This is your PRIORITY
4. **Write the proposal** ‚Äî Use this plan as your blueprint
5. **Submit early** ‚Äî Don't wait until the last day!

---

*Last updated: 2026-02-20*
*Project: AI-Powered Funding Intelligence (FOA Ingestion + Semantic Tagging)*
*Organization: Human AI Foundation / ISSR / University of Alabama*
